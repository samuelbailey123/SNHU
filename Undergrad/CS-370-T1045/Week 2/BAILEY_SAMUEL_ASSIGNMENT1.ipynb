{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#CASE 1\n",
    "#IMPORTS\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "\n",
    "np.random.seed(1671) # for reproducibility # network and training \n",
    "NB_EPOCH=20 \n",
    "BATCH_SIZE=128 \n",
    "VERBOSE=1 \n",
    "NB_CLASSES=10 # NUMBER OF OUTPUTS = NUMBER OF DIGITS \n",
    "OPTIMIZER=SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data ()\n",
    "RESHAPED=784\n",
    "X_train=X_train.reshape(60000, RESHAPED)\n",
    "X_test=X_test.reshape(10000, RESHAPED)\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32') # normalize \n",
    "X_train/= 255 \n",
    "X_test/= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') # convert class vectors to binary class matrices \n",
    "Y_train=np_utils.to_categorical(Y_train, NB_CLASSES) \n",
    "Y_test=np_utils.to_categorical(Y_test, NB_CLASSES) # M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model=Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "optimizer=OPTIMIZER, \n",
    "metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 8s 165us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 8s 168us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 7s 145us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 1s 50us/step\n",
      "Test score: 0.18599770574718713\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#CASE 2\n",
    "#IMPORTS\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility # network and training \n",
    "NB_EPOCH=30 \n",
    "BATCH_SIZE=128 \n",
    "VERBOSE=1 \n",
    "NB_CLASSES=10 # NUMBER OF OUTPUTS = NUMBER OF DIGITS \n",
    "OPTIMIZER=SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data ()\n",
    "RESHAPED=784\n",
    "X_train=X_train.reshape(60000, RESHAPED)\n",
    "X_test=X_test.reshape(10000, RESHAPED)\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32') # normalize \n",
    "X_train/= 255 \n",
    "X_test/= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') # convert class vectors to binary class matrices \n",
    "Y_train=np_utils.to_categorical(Y_train, NB_CLASSES) \n",
    "Y_test=np_utils.to_categorical(Y_test, NB_CLASSES) # M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model=Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "optimizer=OPTIMIZER, \n",
    "metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 1.4881 - accuracy: 0.6275 - val_loss: 0.7539 - val_accuracy: 0.8407\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.5994 - accuracy: 0.8511 - val_loss: 0.4538 - val_accuracy: 0.8841\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.4392 - accuracy: 0.8807 - val_loss: 0.3739 - val_accuracy: 0.8972\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3789 - accuracy: 0.8947 - val_loss: 0.3362 - val_accuracy: 0.9061\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3447 - accuracy: 0.9029 - val_loss: 0.3105 - val_accuracy: 0.9111\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.3209 - accuracy: 0.9093 - val_loss: 0.2931 - val_accuracy: 0.9142\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3024 - accuracy: 0.9143 - val_loss: 0.2781 - val_accuracy: 0.9198\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2873 - accuracy: 0.9187 - val_loss: 0.2657 - val_accuracy: 0.9227\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2749 - accuracy: 0.9215 - val_loss: 0.2560 - val_accuracy: 0.9263\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2638 - accuracy: 0.9250 - val_loss: 0.2488 - val_accuracy: 0.9284\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2538 - accuracy: 0.9278 - val_loss: 0.2396 - val_accuracy: 0.9298\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2446 - accuracy: 0.9295 - val_loss: 0.2329 - val_accuracy: 0.9334\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2364 - accuracy: 0.9320 - val_loss: 0.2281 - val_accuracy: 0.9352\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2291 - accuracy: 0.9338 - val_loss: 0.2204 - val_accuracy: 0.9382\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2219 - accuracy: 0.9362 - val_loss: 0.2139 - val_accuracy: 0.9391\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2153 - accuracy: 0.9382 - val_loss: 0.2088 - val_accuracy: 0.9417\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2091 - accuracy: 0.9398 - val_loss: 0.2039 - val_accuracy: 0.9434\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2032 - accuracy: 0.9413 - val_loss: 0.1995 - val_accuracy: 0.9432\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1978 - accuracy: 0.9429 - val_loss: 0.1956 - val_accuracy: 0.9460\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.1926 - accuracy: 0.9445 - val_loss: 0.1911 - val_accuracy: 0.9463\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1876 - accuracy: 0.9466 - val_loss: 0.1878 - val_accuracy: 0.9482\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.1828 - accuracy: 0.9476 - val_loss: 0.1851 - val_accuracy: 0.9477\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1786 - accuracy: 0.9485 - val_loss: 0.1815 - val_accuracy: 0.9494\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.1745 - accuracy: 0.9498 - val_loss: 0.1784 - val_accuracy: 0.9504\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1702 - accuracy: 0.9511 - val_loss: 0.1747 - val_accuracy: 0.9521\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.1663 - accuracy: 0.9524 - val_loss: 0.1722 - val_accuracy: 0.9531\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.1626 - accuracy: 0.9533 - val_loss: 0.1697 - val_accuracy: 0.9534\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.1589 - accuracy: 0.9545 - val_loss: 0.1667 - val_accuracy: 0.9541\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.1555 - accuracy: 0.9551 - val_loss: 0.1633 - val_accuracy: 0.9557\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.1522 - accuracy: 0.9564 - val_loss: 0.1614 - val_accuracy: 0.9559\n",
      "10000/10000 [==============================] - 0s 34us/step\n",
      "Test score: 0.15738791230246424\n",
      "Test accuracy: 0.9545000195503235\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#CASE 3\n",
    "#IMPORTS\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility # network and training \n",
    "NB_EPOCH=40 \n",
    "BATCH_SIZE=128 \n",
    "VERBOSE=1 \n",
    "NB_CLASSES=10 # NUMBER OF OUTPUTS = NUMBER OF DIGITS \n",
    "OPTIMIZER=SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data ()\n",
    "RESHAPED=784\n",
    "X_train=X_train.reshape(60000, RESHAPED)\n",
    "X_test=X_test.reshape(10000, RESHAPED)\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32') # normalize \n",
    "X_train/= 255 \n",
    "X_test/= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') # convert class vectors to binary class matrices \n",
    "Y_train=np_utils.to_categorical(Y_train, NB_CLASSES) \n",
    "Y_test=np_utils.to_categorical(Y_test, NB_CLASSES) # M_HIDDEN hidden layers # 10 outputs # final stage is softmax \n",
    "model=Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "optimizer=OPTIMIZER, \n",
    "metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1]) \n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 1.5108 - accuracy: 0.6363 - val_loss: 0.7513 - val_accuracy: 0.8436\n",
      "Epoch 2/40\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.5880 - accuracy: 0.8579 - val_loss: 0.4420 - val_accuracy: 0.8880\n",
      "Epoch 3/40\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.4302 - accuracy: 0.8838 - val_loss: 0.3668 - val_accuracy: 0.9010\n",
      "Epoch 4/40\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3742 - accuracy: 0.8950 - val_loss: 0.3332 - val_accuracy: 0.9078\n",
      "Epoch 5/40\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3428 - accuracy: 0.9036 - val_loss: 0.3100 - val_accuracy: 0.9128\n",
      "Epoch 6/40\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3209 - accuracy: 0.9092 - val_loss: 0.2942 - val_accuracy: 0.9171\n",
      "Epoch 7/40\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3037 - accuracy: 0.9137 - val_loss: 0.2800 - val_accuracy: 0.9207\n",
      "Epoch 8/40\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2892 - accuracy: 0.9181 - val_loss: 0.2683 - val_accuracy: 0.9244\n",
      "Epoch 9/40\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2771 - accuracy: 0.9211 - val_loss: 0.2585 - val_accuracy: 0.9264\n",
      "Epoch 10/40\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2659 - accuracy: 0.9249 - val_loss: 0.2514 - val_accuracy: 0.9277\n",
      "Epoch 11/40\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2557 - accuracy: 0.9274 - val_loss: 0.2413 - val_accuracy: 0.9312\n",
      "Epoch 12/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2461 - accuracy: 0.9298 - val_loss: 0.2340 - val_accuracy: 0.9337\n",
      "Epoch 13/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2375 - accuracy: 0.9326 - val_loss: 0.2296 - val_accuracy: 0.9363\n",
      "Epoch 14/40\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2297 - accuracy: 0.9353 - val_loss: 0.2211 - val_accuracy: 0.9388\n",
      "Epoch 15/40\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.2221 - accuracy: 0.9372 - val_loss: 0.2142 - val_accuracy: 0.9401\n",
      "Epoch 16/40\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2151 - accuracy: 0.9390 - val_loss: 0.2088 - val_accuracy: 0.9419\n",
      "Epoch 17/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2086 - accuracy: 0.9409 - val_loss: 0.2034 - val_accuracy: 0.9437\n",
      "Epoch 18/40\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.2022 - accuracy: 0.9427 - val_loss: 0.1985 - val_accuracy: 0.9448\n",
      "Epoch 19/40\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.1964 - accuracy: 0.9441 - val_loss: 0.1945 - val_accuracy: 0.9448\n",
      "Epoch 20/40\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.1910 - accuracy: 0.9460 - val_loss: 0.1898 - val_accuracy: 0.9472\n",
      "Epoch 21/40\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1856 - accuracy: 0.9474 - val_loss: 0.1855 - val_accuracy: 0.9481\n",
      "Epoch 22/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1804 - accuracy: 0.9493 - val_loss: 0.1829 - val_accuracy: 0.9484\n",
      "Epoch 23/40\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.1760 - accuracy: 0.9500 - val_loss: 0.1789 - val_accuracy: 0.9487\n",
      "Epoch 24/40\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1716 - accuracy: 0.9512 - val_loss: 0.1749 - val_accuracy: 0.9516\n",
      "Epoch 25/40\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.1670 - accuracy: 0.9528 - val_loss: 0.1719 - val_accuracy: 0.9513\n",
      "Epoch 26/40\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1630 - accuracy: 0.9538 - val_loss: 0.1685 - val_accuracy: 0.9525\n",
      "Epoch 27/40\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1590 - accuracy: 0.9546 - val_loss: 0.1657 - val_accuracy: 0.9534\n",
      "Epoch 28/40\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1551 - accuracy: 0.9556 - val_loss: 0.1627 - val_accuracy: 0.9544\n",
      "Epoch 29/40\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1516 - accuracy: 0.9568 - val_loss: 0.1595 - val_accuracy: 0.9548\n",
      "Epoch 30/40\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1481 - accuracy: 0.9578 - val_loss: 0.1572 - val_accuracy: 0.9562\n",
      "Epoch 31/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1448 - accuracy: 0.9590 - val_loss: 0.1544 - val_accuracy: 0.9576\n",
      "Epoch 32/40\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1416 - accuracy: 0.9599 - val_loss: 0.1532 - val_accuracy: 0.9572\n",
      "Epoch 33/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1387 - accuracy: 0.9610 - val_loss: 0.1507 - val_accuracy: 0.9580\n",
      "Epoch 34/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1358 - accuracy: 0.9613 - val_loss: 0.1491 - val_accuracy: 0.9581\n",
      "Epoch 35/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1330 - accuracy: 0.9618 - val_loss: 0.1462 - val_accuracy: 0.9589\n",
      "Epoch 36/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1302 - accuracy: 0.9628 - val_loss: 0.1447 - val_accuracy: 0.9592\n",
      "Epoch 37/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1276 - accuracy: 0.9638 - val_loss: 0.1427 - val_accuracy: 0.9595\n",
      "Epoch 38/40\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.1250 - accuracy: 0.9646 - val_loss: 0.1416 - val_accuracy: 0.9599\n",
      "Epoch 39/40\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1226 - accuracy: 0.9654 - val_loss: 0.1389 - val_accuracy: 0.9611\n",
      "Epoch 40/40\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.1202 - accuracy: 0.9661 - val_loss: 0.1375 - val_accuracy: 0.9610\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Test score: 0.13314560524746777\n",
      "Test accuracy: 0.9611999988555908\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "In this assignment we were asked to us the Keras Library and create a network boundary of data to give accuracy testing results.  \n",
    "What I did for my data change was the Batch_Size from 20 - 30 - 40 population size and ran the accuracy tests and training population.\n",
    "When I changed out the different batch sizes to bigger populations I figured that the accuracy of the tests would get greater as \n",
    "the chance for similarities would increase.  While the lower number populations the accuracy could be slimmer shooting than the \n",
    "40 population set.  My test scores per population were for the 20 = 94.6 accuracy , 30 = 95.4 accuracy , 40 = 96.11 accuracy\n",
    "As shown the range isnt anything major but increasing the bath population by 10 in a sense increased the accuracy by 1 percent."
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}