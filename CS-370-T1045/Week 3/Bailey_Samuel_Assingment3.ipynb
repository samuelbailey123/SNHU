{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.7351 - accuracy: 0.3823 - val_loss: 1.4394 - val_accuracy: 0.4872\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.3956 - accuracy: 0.5035 - val_loss: 1.3294 - val_accuracy: 0.5353\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.2626 - accuracy: 0.5538 - val_loss: 1.1824 - val_accuracy: 0.5840\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 1.1737 - accuracy: 0.5863 - val_loss: 1.1692 - val_accuracy: 0.5946\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 1.1040 - accuracy: 0.6115 - val_loss: 1.1401 - val_accuracy: 0.6018\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 1.0415 - accuracy: 0.6340 - val_loss: 1.0902 - val_accuracy: 0.6201\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.9907 - accuracy: 0.6519 - val_loss: 1.0636 - val_accuracy: 0.6328\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.9458 - accuracy: 0.6689 - val_loss: 1.0895 - val_accuracy: 0.6282\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.9060 - accuracy: 0.6848 - val_loss: 1.0834 - val_accuracy: 0.6330\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.8619 - accuracy: 0.6982 - val_loss: 1.0102 - val_accuracy: 0.6578\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.8211 - accuracy: 0.7161 - val_loss: 0.9959 - val_accuracy: 0.6697\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7995 - accuracy: 0.7241 - val_loss: 1.0987 - val_accuracy: 0.6453\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7703 - accuracy: 0.7329 - val_loss: 1.0272 - val_accuracy: 0.6628\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7363 - accuracy: 0.7454 - val_loss: 1.0437 - val_accuracy: 0.6647\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.7134 - accuracy: 0.7529 - val_loss: 0.9906 - val_accuracy: 0.6686\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.6882 - accuracy: 0.7616 - val_loss: 1.0349 - val_accuracy: 0.6667\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 0.6641 - accuracy: 0.7709 - val_loss: 1.0437 - val_accuracy: 0.6692\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.6454 - accuracy: 0.7768 - val_loss: 1.0440 - val_accuracy: 0.6736\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.6254 - accuracy: 0.7847 - val_loss: 1.0746 - val_accuracy: 0.6705\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.6066 - accuracy: 0.7909 - val_loss: 1.0493 - val_accuracy: 0.6672\n",
      "10000/10000 [==============================] - 7s 690us/step\n",
      "Test score: 1.0439956377029418\n",
      "Test accuracy: 0.6592000126838684\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10 \n",
    "from keras.utils import np_utils \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from keras.optimizers import SGD, Adam, RMSprop \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# CIFAR_10 is a set \n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32 \n",
    "IMG_COLS = 32 \n",
    "#constant \n",
    "BATCH_SIZE = 128 \n",
    "NB_EPOCH = 20 \n",
    "NB_CLASSES = 10 \n",
    "VERBOSE = 1 \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "OPTIM = RMSprop() \n",
    "\n",
    "#load dataset \n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "print('X_train shape:', X_train.shape) \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# float and normalization \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255\n",
    "\n",
    "# network \n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3, 3), padding='same', \n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary()\n",
    "\n",
    "# train \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, \n",
    "          epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, \n",
    "          verbose=VERBOSE) \n",
    "score = model.evaluate(X_test, Y_test, \n",
    "                       batch_size=BATCH_SIZE, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model \n",
    "model_json = model.to_json() \n",
    "open('cifar10_architecture.json', 'w').write(model_json) \n",
    "#And the weights learned by our deep network on the training set\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 100s 3ms/step - loss: 1.7891 - accuracy: 0.3520 - val_loss: 1.5256 - val_accuracy: 0.4633\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 1.3322 - accuracy: 0.5241 - val_loss: 1.1323 - val_accuracy: 0.6040\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 100s 2ms/step - loss: 1.1214 - accuracy: 0.6062 - val_loss: 1.0290 - val_accuracy: 0.6328\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 100s 2ms/step - loss: 0.9841 - accuracy: 0.6579 - val_loss: 0.8821 - val_accuracy: 0.6931\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.8886 - accuracy: 0.6891 - val_loss: 0.8076 - val_accuracy: 0.7216\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.8135 - accuracy: 0.7170 - val_loss: 0.8071 - val_accuracy: 0.7200\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.7557 - accuracy: 0.7389 - val_loss: 0.7102 - val_accuracy: 0.7547\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.7110 - accuracy: 0.7523 - val_loss: 0.8250 - val_accuracy: 0.7227\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.6680 - accuracy: 0.7671 - val_loss: 0.7983 - val_accuracy: 0.7380\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.6383 - accuracy: 0.7792 - val_loss: 0.7591 - val_accuracy: 0.7461\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.6089 - accuracy: 0.7906 - val_loss: 0.8387 - val_accuracy: 0.7395\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 100s 2ms/step - loss: 0.5887 - accuracy: 0.7952 - val_loss: 0.7268 - val_accuracy: 0.7673\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 0.5667 - accuracy: 0.8065 - val_loss: 0.7625 - val_accuracy: 0.7655\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5579 - accuracy: 0.8098 - val_loss: 0.7273 - val_accuracy: 0.7587\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5512 - accuracy: 0.8112 - val_loss: 0.7515 - val_accuracy: 0.7849\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5431 - accuracy: 0.8161 - val_loss: 0.7167 - val_accuracy: 0.7729\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5337 - accuracy: 0.8191 - val_loss: 0.7540 - val_accuracy: 0.7709\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5256 - accuracy: 0.8209 - val_loss: 0.7077 - val_accuracy: 0.7733\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5252 - accuracy: 0.8242 - val_loss: 0.8365 - val_accuracy: 0.7922\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 99s 2ms/step - loss: 0.5132 - accuracy: 0.8296 - val_loss: 0.6913 - val_accuracy: 0.7898\n",
      "10000/10000 [==============================] - 9s 925us/step\n",
      "Test score: 0.7264692248344421\n",
      "Test accuracy: 0.7856000065803528\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10 \n",
    "from keras.utils import np_utils \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from keras.optimizers import SGD, Adam, RMSprop \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# CIFAR_10 is a set \n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32 \n",
    "IMG_COLS = 32 \n",
    "#constant \n",
    "BATCH_SIZE = 128 \n",
    "NB_EPOCH = 20 \n",
    "NB_CLASSES = 10 \n",
    "VERBOSE = 1 \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "OPTIM = RMSprop() \n",
    "\n",
    "#load dataset \n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "print('X_train shape:', X_train.shape) \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert to categorical \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# float and normalization \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255\n",
    "\n",
    "# network \n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3, 3), padding='same', \n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Conv2D(32, (3, 3), padding='same')) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "model.add(Conv2D(64, (3, 3), padding='same')) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Conv2D(64, 3, 3)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25)) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary()\n",
    "\n",
    "# train \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, \n",
    "          epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, \n",
    "          verbose=VERBOSE) \n",
    "score = model.evaluate(X_test, Y_test, \n",
    "                       batch_size=BATCH_SIZE, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model \n",
    "model_json = model.to_json() \n",
    "open('cifar10_architecture.json', 'w').write(model_json) \n",
    "#And the weights learned by our deep network on the training set\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Second Test:\n",
    "Test Score = 0.7264692248344421\n",
    "Test Accuracy = 0.7856000065823528\n",
    "\n",
    "First Test:\n",
    "Test Score = 1.0439956377029418\n",
    "Test accuracy: 0.6592000126838684\n",
    "\n",
    "    When looking into this algorithm, i feel it can be used in pretty much any database, especially identifying peoples faces. \n",
    "    Tweaking the algorithm in a way that can value features of a person and can help navigate the data set to find people who \n",
    "    have a similar apperance to the search filters.  This does provide an ethical issue as it makes everyone vulnerable for someone\n",
    "    to come in and search anything about anyone's features.  In the tests above, the first one was running off a massive data set that \n",
    "    gave us an accuracy of 0.659 out of 4,200,080 paramaters to narrow the model we were trying to sample.  The second test was much \n",
    "    more accurate as it came in around 0.786 out of 50,000 train samples.  Limiting the test sample gives a more accuract pull of the\n",
    "    data and can definetly be used in another sequence for any other data set.  Ultimately the developer has to be aware of what the\n",
    "    code is trying to do and not leave people vulnerable and expose them on purpose.  If the developer has a biased thought process,\n",
    "    the data set will be skewed to that performance of the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
